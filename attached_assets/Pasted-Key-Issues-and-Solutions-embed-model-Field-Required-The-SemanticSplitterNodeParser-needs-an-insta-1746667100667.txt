Key Issues and Solutions:

embed_model Field Required:
The SemanticSplitterNodeParser needs an instance of an embedding model (e.g., from OpenAI, HuggingFace). You weren't providing one.
Solution: We'll instantiate OpenAIEmbedding (as it's a common default for LlamaIndex semantic operations) and pass it. We'll also add error handling for its initialization (e.g., if OPENAI_API_KEY is missing).
breakpoint_percentile_threshold Incorrect Type/Value:
The SemanticSplitterNodeParser expects this to be an integer (e.g., 95), representing the percentile. Your code was passing params.breakpoint_percentile_threshold / 100.0, which resulted in a float (e.g., 0.95).
Solution: Pass params.breakpoint_percentile_threshold directly as an integer. Your SemanticSplitterParams model correctly defines it as an int.
Misuse of buffer_size for max_chunk_size:
Your code was passing params.max_chunk_size to the buffer_size argument of SemanticSplitterNodeParser.
buffer_size in SemanticSplitterNodeParser refers to "the number of sentences to group together when computing embedding similarity" (default is 1).
params.max_chunk_size is intended as a token limit for chunks.
Solution: To respect params.max_chunk_size, we should use it to configure an initial SentenceSplitter. This SentenceSplitter's split_text method will then be passed to SemanticSplitterNodeParser's sentence_splitter argument. This pre-splits the text into segments that are (roughly) within the max_chunk_size before semantic analysis occurs. We'll use the default buffer_size for SemanticSplitterNodeParser.
Here's the modified code:

First, update your import section to handle LlamaIndex components and the specific OpenAIEmbedding more robustly:

import logging
import re
from enum import Enum
from typing import Dict, List, Any, Optional, Union, Literal, Callable
import tiktoken
from pydantic import BaseModel, Field

# --- MODIFIED IMPORT SECTION ---
LLAMA_INDEX_AVAILABLE = False
OPENAI_EMBEDDING_AVAILABLE = False

# Define placeholders for types that might not be imported.
# This helps with static analysis and prevents NameErrors if imports fail.
# These will be overwritten by actual classes if imports succeed.
Document, TokenTextSplitter, SentenceSplitter, MarkdownNodeParser, SemanticSplitterNodeParser, OpenAIEmbedding = (None,) * 6
BaseEmbedding = None # For type hints if needed later

try:
    from llama_index.core.node_parser import (
        TokenTextSplitter as CoreTokenTextSplitter,
        SentenceSplitter as CoreSentenceSplitter,
        MarkdownNodeParser as CoreMarkdownNodeParser,
        SemanticSplitterNodeParser as CoreSemanticSplitterNodeParser
    )
    from llama_index.core.schema import Document as CoreDocument
    # from llama_index.core.embeddings.base import BaseEmbedding as CoreBaseEmbedding # If needed for typing

    LLAMA_INDEX_AVAILABLE = True  # Core components are available
    logging.info("LlamaIndex core components successfully imported.")

    # Assign imported classes to the global names
    Document = CoreDocument
    TokenTextSplitter = CoreTokenTextSplitter
    SentenceSplitter = CoreSentenceSplitter
    MarkdownNodeParser = CoreMarkdownNodeParser
    SemanticSplitterNodeParser = CoreSemanticSplitterNodeParser
    # BaseEmbedding = CoreBaseEmbedding

    # Try importing specific embedding model for semantic chunking
    # This requires the 'llama-index-embeddings-openai' package
    try:
        from llama_index.embeddings.openai import OpenAIEmbedding as ImportedOpenAIEmbedding
        OpenAIEmbedding = ImportedOpenAIEmbedding  # Assign to global name
        OPENAI_EMBEDDING_AVAILABLE = True
        logging.info("OpenAIEmbedding successfully imported.")
    except ImportError:
        # OpenAIEmbedding remains None
        logging.warning(
            "OpenAIEmbedding could not be imported from llama_index.embeddings.openai. "
            "Semantic chunking with OpenAI embeddings will not be available. "
            "Ensure 'llama-index-embeddings-openai' is installed if you intend to use it."
        )

except ImportError as e:
    # This means llama_index.core components (or a fundamental part) failed to import
    logging.warning(f"LlamaIndex core components not available: {str(e)}. Some chunking strategies will not be available.")
    # LLAMA_INDEX_AVAILABLE and OPENAI_EMBEDDING_AVAILABLE remain False
    # Placeholders (None) remain assigned
# --- END OF MODIFIED IMPORT SECTION ---


# Define the Chunk model
class Chunk(BaseModel):
    """
    Represents a chunk of text with metadata and statistics.
    """
    chunk_text: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    char_count: int
    token_count: int
    chunk_id: Optional[str] = None

# Parameter models for different chunking strategies
class SimpleSplitterParams(BaseModel):
    """Parameters for simple recursive text splitter."""
    strategy_type: Literal["SIMPLE_RECURSIVE_SPLITTER"] = "SIMPLE_RECURSIVE_SPLITTER"
    chunk_size: int = Field(default=512, gt=0, description="Target chunk size in tokens")
    chunk_overlap: int = Field(default=50, ge=0, description="Number of overlapping tokens between chunks")
    split_method: Literal["token", "sentence"] = Field(
        default="token", 
        description="Split by token count or by sentences"
    )

class SemanticSplitterParams(BaseModel):
    """Parameters for semantic text splitter."""
    strategy_type: Literal["SEMANTIC_CHUNKING_LLAMAINDEX"] = "SEMANTIC_CHUNKING_LLAMAINDEX"
    max_chunk_size: int = Field(default=512, gt=0, description="Maximum chunk size in tokens - chunks may be smaller based on semantic breakpoints")
    breakpoint_percentile_threshold: int = Field(
        default=95, ge=0, le=100, # This is correctly an INT
        description="Percentile threshold for determining breakpoints (higher = fewer but more significant breakpoints)"
    )

class MarkdownSplitterParams(BaseModel):
    """Parameters for Markdown node parser."""
    strategy_type: Literal["MARKDOWN_NODE_PARSER"] = "MARKDOWN_NODE_PARSER"
    chunk_size: int = Field(default=512, gt=0, description="Target chunk size in tokens") # Note: MarkdownNodeParser doesn't directly use chunk_size.

# Chunking Strategy Enum (no changes needed here)
# ... (your existing Enum code) ...

# count_tokens function (no changes needed here)
# ... (your existing count_tokens code) ...

# chunk_with_simple_recursive function (no changes needed here)
# ... (your existing chunk_with_simple_recursive code) ...
Use code with caution.
Python
Now, the corrected chunk_with_semantic function:

def chunk_with_semantic(text: str, params: SemanticSplitterParams) -> List[Chunk]:
    """
    Chunk text using LlamaIndex's semantic splitter.
    
    Args:
        text: Text to chunk
        params: Parameters for the chunking strategy
        
    Returns:
        List[Chunk]: List of generated chunks
    """
    if not LLAMA_INDEX_AVAILABLE:
        raise ValueError("LlamaIndex is not available. Cannot use this chunking strategy.")
    
    # Check if OpenAIEmbedding specifically is available (imported correctly)
    if not OPENAI_EMBEDDING_AVAILABLE or OpenAIEmbedding is None:
        raise ImportError( # Changed from ValueError to ImportError for clarity
            "OpenAIEmbedding is required for this semantic chunking strategy but is not available. "
            "Please ensure 'llama-index-embeddings-openai' is installed and LlamaIndex was imported successfully."
        )

    chunks = []
    
    try:
        document = Document(text=text) # Assumes Document class is available
        
        # 1. Initialize Embedding Model
        embed_model_instance: Optional[BaseEmbedding] = None # Type hint
        try:
            # TODO: Make embed_model configurable (e.g., from strategy params or global settings)
            # For now, assumes OpenAI and API key is set in environment.
            embed_model_instance = OpenAIEmbedding() # Assumes OpenAIEmbedding class is available
        except Exception as e:
            # This catches errors during OpenAIEmbedding() instantiation, e.g., missing API key
            logging.error(
                f"Failed to initialize OpenAIEmbedding for SemanticSplitter: {e}. "
                "Ensure OPENAI_API_KEY is set in your environment and 'llama-index-embeddings-openai' is installed."
            )
            raise ValueError( # Re-raise as ValueError to be caught by the outer try-except if desired
                f"Failed to initialize OpenAI embedding model: {e}. "
                "Please check your API key and model configuration."
            ) from e

        # 2. Create an underlying sentence splitter.
        # This helps to pre-segment the text into manageable "sentences" or initial segments
        # respecting params.max_chunk_size before semantic analysis.
        # SemanticSplitterNodeParser will use this to get initial sentence list.
        # Assumes SentenceSplitter class is available
        initial_sentence_splitter_for_semantic = SentenceSplitter(
            chunk_size=params.max_chunk_size,
            # Default chunk_overlap for SentenceSplitter is 20. This provides some
            # context for the initial "sentences" passed to semantic analysis.
            # If no overlap is desired at this stage, set chunk_overlap=0.
            # We'll stick to SentenceSplitter's default overlap for now.
        )

        # 3. Create SemanticSplitterNodeParser
        # Assumes SemanticSplitterNodeParser class is available
        splitter = SemanticSplitterNodeParser(
            embed_model=embed_model_instance,
            breakpoint_percentile_threshold=params.breakpoint_percentile_threshold, # Pass as int (e.g., 95)
            sentence_splitter=initial_sentence_splitter_for_semantic.split_text, # Pass the method
            # buffer_size: (default=1) Number of "sentences" (from sentence_splitter)
            # to group for embedding comparison. We'll use the default.
            # The user's params.max_chunk_size is handled by the sentence_splitter.
        )
        
        nodes = splitter.get_nodes_from_documents([document])
        
        for i, node in enumerate(nodes):
            chunk_text = node.text
            metadata = dict(node.metadata) if hasattr(node, 'metadata') else {}
            metadata.update({
                "chunk_index": i,
                "splitter": "semantic",
                "breakpoint_percentile": params.breakpoint_percentile_threshold,
                "configured_max_chunk_size_for_pre_split": params.max_chunk_size # Clarified metadata key
            })
            
            token_count = count_tokens(chunk_text)
            char_count = len(chunk_text)
            
            chunk = Chunk(
                chunk_text=chunk_text,
                metadata=metadata,
                char_count=char_count,
                token_count=token_count,
                chunk_id=f"semantic-{i}"
            )
            chunks.append(chunk)
            
    except Exception as e:
        # Catch any other errors during the process, including Pydantic validation if params are wrong
        logging.error(f"Error in semantic chunking: {e}", exc_info=True) # Add exc_info for full traceback
        raise ValueError(f"Failed to chunk text with semantic splitter: {e}") from e # Re-raise with context
    
    return chunks

# chunk_with_markdown_parser function (no changes needed here, but be mindful of chunk_size param)
# ... (your existing chunk_with_markdown_parser code) ...
# Note: MarkdownNodeParser itself doesn't take a chunk_size parameter directly in its constructor.
# It splits based on markdown structure. If you want to further split the nodes it produces by size,
# you'd typically apply another splitter (like TokenTextSplitter) to the nodes from MarkdownNodeParser.
# Your current implementation of chunk_with_markdown_parser does not seem to use params.chunk_size.

# CHUNKER_FUNCTIONS (no changes needed here)
# ...

# get_chunker_function (no changes needed here)
# ...

# calculate_chunk_statistics (no changes needed here)
# ...
Use code with caution.
Python
To make this work, ensure you have the necessary LlamaIndex packages installed:

pip install llama-index llama-index-embeddings-openai tiktoken
Use code with caution.
Bash
And make sure your OPENAI_API_KEY environment variable is set if you're using OpenAIEmbedding.

These changes should resolve the Pydantic validation errors and correctly configure the SemanticSplitterNodeParser.