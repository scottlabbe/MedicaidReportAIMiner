Plan: Chunking Strategy Comparison Tool

Overall Goal:
Extend the admin application to allow processing a given report's text (likely the "clean" text after your chosen PDF parsing method) with two user-selected chunking strategies. The resulting chunks, their metadata, and potentially some summary statistics should be displayed side-by-side for comparison.

I. Backend Modifications (Python/FastAPI):

Define Chunking Strategies (Enum & Implementations):
Create a Python Enum named ChunkingStrategy for available methods.
Implement a Python function for each strategy. Each function will:
Accept the input text string (of an entire report).
Accept any strategy-specific parameters (e.g., chunk size, overlap, model for semantic chunking).
Return a list of "Chunk Objects." Each Chunk Object should be a Pydantic model (or dictionary) containing:
chunk_text: str (The actual text of the chunk)
metadata: dict (Any metadata generated by the chunker, e.g., sentence range, source paragraph, section headers if identifiable. This will be crucial for what gets stored in the vector DB).
char_count: int (Length of chunk_text)
token_count: int (Estimated token count of chunk_text - using a tokenizer like tiktoken)
(Optional) chunk_id: str (A unique ID for the chunk within this processing run)
Specific Strategies to Implement:
SIMPLE_RECURSIVE_SPLITTER:
Use LlamaIndex's TokenTextSplitter or SentenceSplitter (which can be configured to be recursive by trying different separators).
Parameters: chunk_size (in tokens or characters), chunk_overlap.
Name: "Simple Recursive Splitter (LlamaIndex)"
Metadata: Start/end character index from original text, maybe a simple chunk sequence number.
SEMANTIC_CHUNKING_LLAMAINDEX:
Use LlamaIndex's SemanticSplitterNodeParser. This requires an embedding model.
Parameters: embed_model (e.g., OpenAI's, or a local SentenceTransformer), breakpoint_percentile_threshold (or other relevant parameters for the semantic splitter).
Name: "Semantic Chunking (LlamaIndex)"
Metadata: Potentially a "semantic similarity score" if the splitter provides it, chunk sequence.
MARKDOWN_HEADER_SPLITTER (Recommended Additional Strategy):
If your input text could be converted to or is already in Markdown, LlamaIndex's MarkdownHeaderTextSplitter is excellent. It splits based on Markdown headers (H1, H2, etc.), keeping related content together. This often produces semantically coherent chunks.
Alternatively, if not Markdown, a strategy that tries to identify document sections/headings heuristically (e.g., looking for lines in all caps followed by a newline, or lines with specific formatting if your PDF parser can provide that info) and then chunks within those sections.
Name: "Header/Section Aware Splitter"
Metadata: Detected header, section hierarchy.
(Alternative to LlamaIndex Semantic Chunking, if you prefer Hugging Face directly):
You could implement semantic chunking using a library like semantic-text-splitter from Hugging Face or by adapting logic from similar projects. This would involve more manual work with embeddings and similarity calculations. For ease of integration, LlamaIndex's abstraction is often simpler to start with.
Endpoint for Initiating Chunking Comparison:
Create a new API endpoint (e.g., /api/reports/{report_id}/compare_chunks).
This endpoint will accept:
report_id: The ID of an already processed and saved report (from which to fetch the clean text).
chunking_strategy_key_1: Identifier for the first chunking strategy.
params_1: A dictionary of parameters for the first strategy (e.g., {"chunk_size": 500, "chunk_overlap": 100}).
chunking_strategy_key_2: Identifier for the second chunking strategy.
params_2: Parameters for the second strategy.
The backend will:
Fetch the clean text of the specified report_id from your database.
Invoke the chunking function for strategy 1 with its parameters, yielding chunk_list_1.
Invoke the chunking function for strategy 2 with its parameters, yielding chunk_list_2.
Calculate summary statistics for each list of chunks (e.g., total number of chunks, average chunk length in characters/tokens, min/max chunk length).
Temporary Data Storage: Store the results (strategy_name_1, params_1, chunk_list_1, summary_stats_1, and similarly for strategy 2, plus the report_id or report_title for context) temporarily, associated with a unique chunk_comparison_id. Use a similar temporary storage mechanism as the parser comparison.
Return a JSON response with the chunk_comparison_id.
Endpoint to Serve Chunking Comparison Data:
Create an API endpoint (e.g., /api/chunk_comparison_results/{chunk_comparison_id}).
Retrieves and returns the temporarily stored chunking results as JSON.
II. Frontend Modifications (HTML/Jinja2 & JavaScript):

UI for Initiating Chunking Comparison:
In your admin dashboard, likely on a report's detail page or a dedicated "RAG Tools" section.
Allow the admin to select an existing report.
Provide two sets of controls (for "Strategy 1" and "Strategy 2"):
Dropdown to select a ChunkingStrategy from the available Enum.
Dynamic input fields to specify parameters relevant to the selected strategy (e.g., if "Simple Recursive" is chosen, show inputs for chunk_size and chunk_overlap; if "Semantic Chunking" is chosen, show relevant semantic parameters). JavaScript will be needed to show/hide parameter fields based on strategy selection.
A "Compare Chunking Strategies" button.
On click, JavaScript collects the report_id, selected strategies, and their parameters, then POSTs to /api/reports/{report_id}/compare_chunks.
On success (receiving chunk_comparison_id), redirect to the new "Chunking Comparison Review" page with this ID.
New "Chunking Comparison Review" Page Template (compare_chunks.html):
On page load, JavaScript fetches data from /api/chunk_comparison_results/{chunk_comparison_id}.
Layout: Two-column side-by-side layout.
Content in Each Column (for Strategy 1 and Strategy 2):
Strategy Info: Display "Strategy: [Name of Chunking Strategy]" and the parameters used (e.g., "Chunk Size: 500, Overlap: 100").
Summary Statistics:
Total Chunks: [Number]
Average Chunk Length (chars): [Number]
Average Chunk Length (tokens): [Number]
Min/Max Chunk Length (tokens): [Min] / [Max]
Chunk List Display:
A scrollable list of the generated chunks.
For each chunk in the list:
Display chunk_text (perhaps truncated with a "show more" option if very long, or in a scrollable sub-element).
Display its metadata (formatted JSON or key-value pairs). This is very important!
Display char_count and token_count.
(Optional) A search/filter box within each column to find specific text within that column's chunks.
Overall View:
Report Title/ID being analyzed.
"Start New Comparison" button.
No "Save" or "Embed" Action: This page is purely for visual comparison and analysis to inform strategy choices. The actual embedding process would happen later, using a chosen strategy as part of your RAG ingestion pipeline.
III. Implementation Notes for AI Model:

Modular Chunking Functions: Stress that each chunk_X_strategy function should be independent.
Parameterization: The ability to pass and use different parameters for each strategy is key. The frontend will need to dynamically show relevant parameter inputs.
Metadata is Key: Emphasize that the metadata associated with each chunk is critical, as this is what will be stored alongside the vector embedding and used for filtering or providing context during RAG retrieval.
Token Counting: Explain the need to integrate a tokenizer (like tiktoken for OpenAI models) to accurately report token counts for chunks, as this is crucial for understanding LLM context window usage and costs.
Temporary Data Handling: Similar to the parser comparison, a robust way to handle temporary storage of comparison results is needed.
Dynamic Frontend for Parameters: Guide the AI on how to use JavaScript to dynamically display input fields for parameters based on the selected chunking strategy in the dropdowns.
Example ChunkingStrategy Enum and Parameter Handling Idea (Conceptual):

from enum import Enum
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Literal

# --- Pydantic Models for Strategy Parameters ---
class SimpleSplitterParams(BaseModel):
    strategy_type: Literal["SIMPLE_RECURSIVE_SPLITTER"] = "SIMPLE_RECURSIVE_SPLITTER"
    chunk_size: int = Field(default=512, gt=0)
    chunk_overlap: int = Field(default=50, ge=0)

class SemanticSplitterParams(BaseModel):
    strategy_type: Literal["SEMANTIC_CHUNKING_LLAMAINDEX"] = "SEMANTIC_CHUNKING_LLAMAINDEX"
    breakpoint_percentile_threshold: int = Field(default=95, ge=0, le=100)
    # embed_model_name: str = "default_openai_embedding" # Or allow selection

class ChunkingStrategy(Enum):
    SIMPLE_RECURSIVE_SPLITTER = ("Simple Recursive Splitter (LlamaIndex)", SimpleSplitterParams)
    SEMANTIC_CHUNKING_LLAMAINDEX = ("Semantic Chunking (LlamaIndex)", SemanticSplitterParams)
    # Add other strategies and their param models

    def __init__(self, display_name, param_model_cls):
        self._display_name = display_name
        self._param_model_cls = param_model_cls

    @property
    def display_name(self):
        return self._display_name

    @property
    def param_model(self):
        return self._param_model_cls

    @classmethod
    def choices(cls):
        return [(member.name, member.display_name) for member in cls]

# --- Chunk Object Model ---
class ChunkOutput(BaseModel):
    chunk_text: str
    metadata: Dict[str, Any]
    char_count: int
    token_count: int

# --- Example Chunking Function (Conceptual) ---
# def chunk_with_simple_splitter(text: str, params: SimpleSplitterParams) -> List[ChunkOutput]:
#     # ... use LlamaIndex TokenTextSplitter with params.chunk_size, params.chunk_overlap
#     # ... calculate metadata, token counts for each chunk
#     return list_of_chunk_outputs